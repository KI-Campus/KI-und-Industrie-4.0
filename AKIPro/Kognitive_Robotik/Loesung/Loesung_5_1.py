solution = str('Verwendete Aktivierungsfunktionen: \n\n "ReLu": Die Aktivierungsfunktion "ReLu" (steht für Rectified Linear Unit) wird für die verdeckten Schichten des Netzes verwendet. Sie ist durch die Gleichung f(x) = max(0,x) gegeben und hat den Wertebereich [0,inf). Sie wird standardmäßig für verdeckte Schichten von NNs verwendet. \n\n "Sigmoid": Für die Ausgabeschicht des Zweigs Lokalisierung wird die Aktivierungsfunktion "Sigmoid" verwendet. Sie ist durch die Gleichung f(x) = 1/(1+exp(-x)) gegeben und hat den Wertebereich [0,1]. Bei Regressionsproblemen wird häufig die Aktivierungsfunktion "Linear", gegeben durch f(x) = x für die Ausgabeschicht verwendet, da die Ausgabewerte hier jeden beliebigen Wert zwischen -inf und inf annehmen können. In dem hier betrachteten Fall sind die Ausgabewerte allerdings auf Werte zwischen 0 und 1 beschränkt und es hat sich gezeigt, dass "Sigmoid" zu einer besseren Performance führt. \n\n "Softmax": Für die Ausgabeschicht des Zweigs Klassifizierung wird die Aktivierungsfunktion "Softmax" verwendet. Sie ist durch die Gleichung f(x_i) = exp(x_i)/sum_j exp(x_j) gegeben, wobei j hier die Neuronen der Ausgabeschicht sind. Durch die Division durch die Summe der exponentiellen Werte, werden die Ausgabewerte normalisiert, sodass sie im Wertebereich [0,1] liegen und die Summe der Ausgabewerte eins ergibt. Die Ausgabewerte können hier als Wahrscheinlichkeiten für die Klassenzugehörigkeit interpretiert werden.') 
print(solution)
